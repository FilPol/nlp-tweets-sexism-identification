{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4vFspE6wAC_"
      },
      "source": [
        "# Static Word Embeddings for Text Representation  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook explores static word embeddings for text representation in English and Spanish. It preprocesses tweet datasets by removing URLs, hashtags, and stopwords before tokenizing the text.\n",
        "\n",
        "Pre-trained embeddings (Word2Vec, FastText, GloVe) are used to compute sentence representations.\n",
        "\n",
        "The cosine similarity between tweet embeddings is calculated to identify the most semantically similar sexist and non-sexist tweets."
      ],
      "metadata": {
        "id": "SWn5IPdszTus"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sE8J46kCONUy",
        "outputId": "eb9726cd-55da-47d8-a820-22e5238c6470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: fasttext in /usr/local/lib/python3.11/dist-packages (0.9.3)\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.11/dist-packages (from fasttext) (2.13.6)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "!pip install -U gensim\n",
        "!pip install -U nltk\n",
        "!pip install -U fasttext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iBKniCGOQ5X",
        "outputId": "242d5a9c-7f22-40d2-9951-c1fb6d137f3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import numpy as np\n",
        "import fasttext.util\n",
        "import gensim\n",
        "import gensim.downloader as api\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "nltk.download(\"punkt_tab\")\n",
        "# nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJzPaO1pwADD"
      },
      "source": [
        "## Load both corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "od0PeFAUwADD",
        "outputId": "b939f567-7a28-4ae4-f782-ed6be4420637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mc_82qG5q2HE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# df = {\n",
        "#    \"english\": \"drive/MyDrive/EXIST2024_EN_examples.csv\",\n",
        "#    \"spanish\": \"drive/MyDrive/EXIST2024_ES_examples.csv\"\n",
        "# }\n",
        "\n",
        "\n",
        "path_english = \"drive/MyDrive/EXIST2024_EN_examples.csv\"\n",
        "path_spanish = \"drive/MyDrive/EXIST2024_ES_examples.csv\"\n",
        "\n",
        "df = {\n",
        "    \"english\": pd.read_csv(path_english, sep=\"\\t\"),\n",
        "    \"spanish\": pd.read_csv(path_spanish, sep=\"\\t\")\n",
        "}\n",
        "spanish_data = df[\"spanish\"]\n",
        "english_data = df[\"english\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WIc7OLcpwADF"
      },
      "outputs": [],
      "source": [
        "web_re = re.compile(r\"https?:\\/\\/[^\\s]+\", re.U)\n",
        "user_re = re.compile(r\"(@\\w+\\-?(?:\\w+)?)\", re.U)\n",
        "hashtag_re = re.compile(r\"(#\\w+\\-?(?:\\w+)?)\", re.U)\n",
        "\n",
        "stopw = {\n",
        "    \"english\": nltk.corpus.stopwords.words(\"english\"),\n",
        "    \"spanish\": nltk.corpus.stopwords.words(\"spanish\")\n",
        "}\n",
        "\n",
        "def preprocess(text):\n",
        "    text = web_re.sub(\"\", text)\n",
        "    text = user_re.sub(\"\", text)\n",
        "    text = hashtag_re.sub(\"\", text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "def tokenize(text_list, lang=\"english\"):\n",
        "    token_list = []\n",
        "    for text in text_list:\n",
        "        text = preprocess(text)\n",
        "        tokens = word_tokenize(text, language=lang)\n",
        "        tokens = [word for word in tokens if word.isalnum() and word not in stopw[lang]]\n",
        "        token_list.append(tokens)\n",
        "    return token_list\n",
        "\n",
        "tokenized_text = {\n",
        "    \"english\": tokenize(df[\"english\"][\"text\"], \"english\"),\n",
        "    \"spanish\": tokenize(df[\"spanish\"][\"text\"], \"spanish\")\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUup9ejVwADF"
      },
      "source": [
        "##¬†Text representation using static embeddings\n",
        "\n",
        "ENGLISH\n",
        "\n",
        "- word2vec-google-news-300 (using Gemini)\n",
        "- fasttext-wiki-news-subwords-300 (using Gemini)\n",
        "- glove-wiki-gigaword-300 (using Gemini)\n",
        "\n",
        "SPANISH\n",
        "- Fasttext (https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz) (using Gemini)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wClIBsfEwADG"
      },
      "source": [
        "###¬†Load the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "UrrZUm2OwADG"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "info = api.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiVNuQWDwADG"
      },
      "source": [
        "### Compute static word-embeddings representation of the tweets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jOfurEycwADH"
      },
      "outputs": [],
      "source": [
        "def gensim_sentence_rep(tokens, model):\n",
        "    avg_vec = np.zeros(model.vector_size)\n",
        "    total_w = 0\n",
        "    for token in tokens:\n",
        "      if token in model:\n",
        "        avg_vec += model.get_vector(token)\n",
        "        total_w += 1\n",
        "\n",
        "    if total_w == 0:\n",
        "      return avg_vec\n",
        "\n",
        "    return avg_vec / total_w"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(models, language):\n",
        "    embeds = {name: [] for model, name in models}\n",
        "\n",
        "    for model, name in models:\n",
        "      for tokens in tokenized_text[language]:\n",
        "        embeds[name].append(gensim_sentence_rep(tokens, model))\n",
        "\n",
        "    return embeds"
      ],
      "metadata": {
        "id": "Q9mwL1Mq1zGR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RautX5QvwADH"
      },
      "source": [
        "##¬†Compute cosine similarities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5bUUaFC0L6PE"
      },
      "outputs": [],
      "source": [
        "def find_closest_similarity(model_embed, tweets, is_sexist):\n",
        "  similarity = np.round(cosine_similarity(model_embed, model_embed), 4)\n",
        "\n",
        "  tri_upper_indices = np.triu_indices_from(similarity, k=1)\n",
        "  max_index = np.argmax(similarity[tri_upper_indices])\n",
        "  tweet_idx1, tweet_idx2 = tri_upper_indices[0][max_index], tri_upper_indices[1][max_index]\n",
        "\n",
        "  label = \"Yes\" if is_sexist else \"NO\"\n",
        "  print(f\"label: {label}\\n sentence1: {tweets.iloc[tweet_idx1]['text']} \\n --------------------\")\n",
        "  print(f\"sentence2: {tweets.iloc[tweet_idx2]['text']} \\n distance: {similarity[tweet_idx1, tweet_idx2]}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4EDMZpmwADH"
      },
      "source": [
        "## Show results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fzvF7cOvQf7p"
      },
      "outputs": [],
      "source": [
        "def show_results(tweets, embeds):\n",
        "  for name, model_embed in embeds.items():\n",
        "    tweets_nonsexist = tweets[tweets[\"label\"] == \"NO\"].reset_index(drop=True)\n",
        "    tweets_sexist = tweets[tweets[\"label\"] == \"YES\"].reset_index(drop=True)\n",
        "\n",
        "    embeds_sexist = np.array([model_embed[i] for i in tweets[tweets[\"label\"] == \"YES\"].index.to_list()])\n",
        "    embeds_nonsexist = np.array([model_embed[i] for i in tweets[tweets[\"label\"] == \"NO\"].index.to_list()])\n",
        "\n",
        "    print(f\"{name}\\n======\\n\")\n",
        "    for tweets, is_sexist, embeddings in [(tweets_nonsexist, False, embeds_nonsexist), (tweets_sexist, True, embeds_sexist)]:\n",
        "        find_closest_similarity(embeddings, tweets, is_sexist)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w2v300 = api.load(\"word2vec-google-news-300\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hyoxfhC52Gqf",
        "outputId": "8a970247-3044-40ef-ffbc-4b1b0b2eb195"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# models = [(w2v300, \"w2v300\"), (ftsub300, \"ftsub300\"), (glwiki300, \"glwiki300\")]\n",
        "models = [(w2v300, \"w2v300\")]\n",
        "embeds = get_embeddings(models, \"english\")\n",
        "show_results(english_data, embeds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVclZ_XJ2SQd",
        "outputId": "309508ef-5ede-4a75-8281-61d921336ed1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w2v300\n",
            "======\n",
            "\n",
            "label: NO\n",
            " sentence1: @BLEEDTHISWAY replay free woman breebylon &gt;&gt;&gt; Flop this way \n",
            " --------------------\n",
            "sentence2: replay&gt;alice&gt;babylon&gt;free woman https://t.co/WCEqeUxdtC \n",
            " distance: 0.9255\n",
            "\n",
            "label: Yes\n",
            " sentence1: @WeaponizedRage Aerosmith in 1987: \"Dude looks like a lady\" \n",
            " --------------------\n",
            "sentence2: Dude does not look like a lady! https://t.co/C62JmKSzy0 \n",
            " distance: 0.9614\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ftsub300 = api.load(\"fasttext-wiki-news-subwords-300\")"
      ],
      "metadata": {
        "id": "FYP7m7CL-kBX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "551771d5-71f6-4a5e-9be3-c91c15638466"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [(ftsub300, \"ftsub300\")]\n",
        "embeds = get_embeddings(models, \"english\")\n",
        "show_results(english_data, embeds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHRPAEAjLtCH",
        "outputId": "a64f2321-e3d3-48f5-e36c-1e3a0a0611c9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftsub300\n",
            "======\n",
            "\n",
            "label: NO\n",
            " sentence1: @ChileMATD @lotusmusica @lollapaloozacl Free woman #MARINAEnChile2022 #SideshowParaMARINA \n",
            " --------------------\n",
            "sentence2: You have the right to be a free woman @antonioguterres @mbachelet @EmmanuelMacron @UNESCO @amnesty @hrw https://t.co/ftQTwQ4izi \n",
            " distance: 0.9541\n",
            "\n",
            "label: Yes\n",
            " sentence1: didn‚Äôt have to do Ethan like that tho‚Ä¶i hate women üòí https://t.co/Zf5y9fsVW2 \n",
            " --------------------\n",
            "sentence2: @mehrospace @hieiishere @greenyankeemanc @RealBetyCardens @LAPDHQ I hate women like youü§£ \n",
            " distance: 0.9678\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glwiki300 = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "metadata": {
        "id": "KaAPLWiZ-Y5R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9835e01c-5f71-49b1-a680-785691e6bcd3"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [(glwiki300, \"glwiki300\")]\n",
        "embeds = get_embeddings(models, \"english\")\n",
        "show_results(english_data, embeds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WHuXlcILxAF",
        "outputId": "9985c389-7e33-4192-9827-acccb7f30f52"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "glwiki300\n",
            "======\n",
            "\n",
            "label: NO\n",
            " sentence1: @TarekFatah @RashidaTlaib @Ilhan Always playing the victim card. \n",
            " --------------------\n",
            "sentence2: Where dvmbses? I don't see anything, are you playing the victim card again? ü•± https://t.co/UCk2kQuWda \n",
            " distance: 0.939\n",
            "\n",
            "label: Yes\n",
            " sentence1: @lkmeenha we can‚Äôt even have a day without women making it about themselves üôÑ \n",
            " --------------------\n",
            "sentence2: @BigDILF01 Can‚Äôt go a day without women womening \n",
            " distance: 0.9474\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Sse6Ass8Qt7W"
      },
      "outputs": [],
      "source": [
        "ftes300 = KeyedVectors.load_word2vec_format(\"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.es.300.vec.gz\", binary=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [(ftes300, \"ftes300\")]\n",
        "embeds = get_embeddings(models, \"spanish\")\n",
        "show_results(spanish_data, embeds)"
      ],
      "metadata": {
        "id": "OEDJQEAr-XuS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b7461d7-0191-4232-f275-13e11eb1cc86"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ftes300\n",
            "======\n",
            "\n",
            "label: NO\n",
            " sentence1: @rufinelix's account is temporarily unavailable because it violates the Twitter Media Policy. Learn more. \n",
            " --------------------\n",
            "sentence2: @Moreno19841's account is temporarily unavailable because it violates the Twitter Media Policy. Learn more. \n",
            " distance: 1.0\n",
            "\n",
            "label: Yes\n",
            " sentence1: ‚òÄÔ∏è Los hombres tambi√©n sufren depresi√≥n postparto... https://t.co/sAdzd9LUrc \n",
            " --------------------\n",
            "sentence2: üçè Los hombres tambi√©n sufren depresi√≥n postparto... https://t.co/OVNEvgr0ZC \n",
            " distance: 1.0\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}