{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlU74rrb_sdB"
   },
   "source": [
    "# Transformers and Explainability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WBsbUgqWkf0e"
   },
   "source": [
    "**Experimental Setup**\n",
    "\n",
    "Pretrained multilingual transformer model was used with Ferretâ€™s Benchmark to generate and evaluate explanations for Spanish text classification\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "Explanations for given sentences and target classes were generated, deduplicated to remove redundancy, and quantitatively evaluated for faithfulness. Results were displayed below, as interactive tables for easy inspection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTULARsX_sdG"
   },
   "source": [
    "## Some libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HUtN3tm7_sdG",
    "outputId": "69296f2a-9f26-4303-d942-9e6d09af0a27"
   },
   "outputs": [],
   "source": [
    "!pip install \\\n",
    "    transformers==4.43.1 \\\n",
    "    sentence-transformers==2.2.2 \\\n",
    "    ferret-xai==0.4.2 \\\n",
    "    scikit-learn==1.4.2 \\\n",
    "    pandas==2.2.2 \\\n",
    "    tqdm==4.66.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTXwg3hgjLjx",
    "outputId": "46fc9431-03d0-4719-cbad-addfad4fc819"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.25.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1UsJHBKwjFkC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6J1hMo-sKHsh"
   },
   "outputs": [],
   "source": [
    "!pip install numpy==1.26.4\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3BH2Brl_sdH"
   },
   "source": [
    "## Import readerEXIST2025 library, and read the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmPgLF25_sdJ",
    "outputId": "8202cb97-9f8e-4aac-fd16-4274e0bd3f96"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "base_path = \"/content/drive/MyDrive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VeDECgqv_sdJ"
   },
   "outputs": [],
   "source": [
    "sys.path.append(base_path)\n",
    "from readerEXIST2025 import EXISTReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Ik6Wm17_sdJ"
   },
   "outputs": [],
   "source": [
    "file_train = os.path.join(base_path, \"EXIST2025_training.json\")\n",
    "file_dev = os.path.join(base_path, \"EXIST2025_dev.json\")\n",
    "\n",
    "reader_train = EXISTReader(file_train)\n",
    "reader_dev = EXISTReader(file_dev)\n",
    "\n",
    "EnTrainTask1, EnDevTask1 = reader_train.get(lang=\"EN\", subtask=\"1\"), reader_dev.get(lang=\"EN\", subtask=\"1\")\n",
    "SpTrainTask1, SpDevTask1 = reader_train.get(lang=\"ES\", subtask=\"1\"), reader_dev.get(lang=\"ES\", subtask=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VR18vXQH_sdJ"
   },
   "source": [
    "## Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SUBOC9h_sdJ"
   },
   "outputs": [],
   "source": [
    "class SexismDataset(Dataset):\n",
    "    def __init__(self, texts, labels, ids, tokenizer, max_len=128, pad=\"max_length\", trunc=True,rt='pt'):\n",
    "        self.texts = texts.tolist()\n",
    "        self.labels = labels\n",
    "        self.ids = ids\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.pad = pad\n",
    "        self.trunc = trunc\n",
    "        self.rt = rt\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,padding=self.pad, truncation=self.trunc,\n",
    "            return_tensors=self.rt\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].flatten(),\n",
    "            'attention_mask': inputs['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            'id': torch.tensor(self.ids[idx], dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4-Ov1wj_sdK"
   },
   "source": [
    "## Auxiliary functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUCjv1vOny1c"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import tee, count\n",
    "\n",
    "def uniquify(seq, suffs = count(1)):\n",
    "\n",
    "    not_unique = [k for k,v in Counter(seq).items() if v > 1]\n",
    "    suff_gens = dict(zip(not_unique, tee(suffs, len(not_unique))))\n",
    "    for idx,s in enumerate(seq):\n",
    "        try:\n",
    "            suffix = str(next(suff_gens[s]))\n",
    "        except KeyError:\n",
    "            continue\n",
    "        else:\n",
    "            seq[idx] += suffix\n",
    "\n",
    "def deduplicate(explanations):\n",
    "    for i in range(len(explanations)):\n",
    "        tokens = explanations[i].tokens\n",
    "        uniquify(tokens, (f'_{x!s}' for x in range(1, 100)))\n",
    "        explanations[i].tokens=tokens\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEbN0F2w_sdK"
   },
   "source": [
    "# Two options to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukH9oraMH1xo"
   },
   "source": [
    "### LORA pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9dfejM-H0WB"
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "def sexism_classification_pipeline_task1_LoRA(trainInfo, devInfo, testInfo=None, model_name='roberta-base', nlabels=2, ptype=\"single_label_classification\", **args):\n",
    "    # Model and Tokenizer\n",
    "    labelEnc = LabelEncoder()\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=nlabels,\n",
    "        problem_type=ptype\n",
    "    )\n",
    "\n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "    task_type= args.get(\"task_type\", TaskType.SEQ_CLS),\n",
    "    target_modules= args.get(\"target_modules\", [\"query\", \"value\"]),\n",
    "    r= args.get(\"rank\", 64),  # Rank of LoRA adaptation\n",
    "    lora_alpha=args.get(\"lora_alpha\", 32),  # Scaling factor\n",
    "    lora_dropout=args.get(\"lora_dropout\", 0.1),\n",
    "    bias=args.get(\"bias\", \"none\")\n",
    ")\n",
    "\n",
    "    # Prepare LoRA model\n",
    "    peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Prepare datasets\n",
    "    train_dataset = SexismDataset(trainInfo[1], labelEnc.fit_transform(trainInfo[2]),[int(x) for x in trainInfo[0]], tokenizer )\n",
    "    val_dataset = SexismDataset(devInfo[1], labelEnc.transform(devInfo[2]), [int(x) for x in devInfo[0]], tokenizer)\n",
    "\n",
    "    # Training Arguments\n",
    "    training_args = TrainingArguments(\n",
    "        report_to=\"none\", # alt: \"wandb\", \"tensorboard\" \"comet_ml\" \"mlflow\" \"clearml\"\n",
    "        output_dir= args.get('output_dir', './results_task1_LoRA0'),\n",
    "        num_train_epochs= args.get('num_train_epochs', 5),\n",
    "        learning_rate=args.get('learning_rate', 5e-5),\n",
    "        per_device_train_batch_size=args.get('per_device_train_batch_size', 16),\n",
    "        per_device_eval_batch_size=args.get('per_device_eval_batch_size', 64),\n",
    "        warmup_steps=args.get('warmup_steps', 500),\n",
    "        weight_decay=args.get('weight_decay',0.01),\n",
    "        logging_dir=args.get('logging_dir', './logs'),\n",
    "        logging_steps=args.get('logging_steps', 10),\n",
    "        eval_strategy=args.get('eval_strategy','epoch'),\n",
    "        save_strategy=args.get('save_strategy', \"epoch\"),\n",
    "        save_total_limit=args.get('save_total_limit', 1),\n",
    "        load_best_model_at_end=args.get('load_best_model_at_end', True),\n",
    "        metric_for_best_model=args.get('metric_for_best_model',\"f1\")\n",
    "    )\n",
    "\n",
    "    # Initialize Trainer\n",
    "    trainer = Trainer(\n",
    "        # Prepare LoRA model\n",
    "        model=peft_model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics_1,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=args.get(\"early_stopping_patience\",3))]\n",
    "    )\n",
    "\n",
    "    # Fine-tune the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    eval_results = trainer.evaluate()\n",
    "    print(\"Validation Results:\", eval_results)\n",
    "\n",
    "    #Saving the new weigths for the LoRA model\n",
    "    # trainer.save_model(dir)\n",
    "    # Notice that, in this case only the LoRA matrices are saved.\n",
    "    # The weigths for the classification head are not saved.\n",
    "\n",
    "    #Mixing the LoRA matrices with the weigths of the base model used\n",
    "    mixModel=peft_model.merge_and_unload()\n",
    "    # mixModel.save_pretrained(dir)\n",
    "    # IN this case the full model is saved.\n",
    "\n",
    "    if testInfo is not None:\n",
    "        # Prepare test dataset for prediction\n",
    "        test_dataset = SexismDataset(testInfo[1], [0] * len(testInfo[1]),  [int(x) for x in testInfo[0]],   tokenizer)\n",
    "\n",
    "        # Predict test set labels\n",
    "        predictions = trainer.predict(test_dataset)\n",
    "        predicted_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "        # Create submission DataFrame\n",
    "        submission_df = pd.DataFrame({\n",
    "            # 'id': testInfo[0],\n",
    "            'label': labelEnc.inverse_transform(predicted_labels),\n",
    "            \"test_case\": [\"EXIST2025\"]*len(predicted_labels)\n",
    "        })\n",
    "        submission_df.to_csv('sexism_predictions_task1.csv', index=False)\n",
    "        print(\"Prediction for TASK 1 completed. Results saved to sexism_predictions_task1.csv\")\n",
    "        return model, submission_df\n",
    "    return model, eval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sU24xpiHJRq_"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r_iPmXYqJCsj"
   },
   "outputs": [],
   "source": [
    "def compute_metrics_1(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='binary', zero_division=0\n",
    "    )\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BRj-hNa1_sdL"
   },
   "source": [
    "### The simplest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ww6HcO7VoVPd"
   },
   "outputs": [],
   "source": [
    "def predict_op1(model, dataset, args = {}):\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./output\",\n",
    "        per_device_eval_batch_size = args.get(\"per_device_eval_batch_size\", 16),\n",
    "        do_train=False,\n",
    "        do_eval=False,\n",
    "    )\n",
    "    trainer = Trainer(model=model, args=training_args)\n",
    "\n",
    "    predictions = trainer.predict(dataset)\n",
    "\n",
    "    logits = predictions.predictions\n",
    "\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(logits), dim=-1).numpy()\n",
    "\n",
    "    pred_classes = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return pred_classes, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gfZgHGIB_sdM"
   },
   "source": [
    "### Predictions from the best Spanish model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "Rj0JFN23TkLs",
    "outputId": "e666942b-1577-4fef-945d-dbac484347a6"
   },
   "outputs": [],
   "source": [
    "from transformers import  AutoTokenizer, AutoModelForSequenceClassification,  Trainer, TrainingArguments,  EarlyStoppingCallback\n",
    "\n",
    "params = {\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"num_train_epochs\": 4,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"early_stopping_patience\": 2,\n",
    "    \"use_lora\": True,\n",
    "}\n",
    "modelname = \"PlanTL-GOB-ES/roberta-base-bne\"\n",
    "output_dir = \"./drive/MyDrive/best-finetuned-model-spanish\"\n",
    "mixmodel, res = sexism_classification_pipeline_task1_LoRA(SpTrainTask1, SpDevTask1, None, modelname, 2, \"single_label_classification\", **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "sKPat5FkIB57",
    "outputId": "a66f5cf1-c8db-4e20-cf8d-2f81046dfc18"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "val_labels_encoded = label_encoder.fit_transform(SpDevTask1[2]) # Assuming SpDevTask1[2] are the string labels\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"PlanTL-GOB-ES/roberta-base-bne\")\n",
    "\n",
    "val_dataset = SexismDataset(SpDevTask1[1], LabelEncoder().fit_transform(SpDevTask1[2]), [int(x) for x in SpDevTask1[0]], tokenizer)\n",
    "\n",
    "preds, probs = predict_op1(mixmodel, val_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSMxFTeq_sdN"
   },
   "source": [
    "### Evaluation of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eaWyVzs1_sdN",
    "outputId": "bff46bb5-4dc5-4b03-a198-09acfc1a1b69"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(y_true, y_pred):\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='binary'\n",
    "    )\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }\n",
    "\n",
    "metrics = compute_metrics(val_dataset.labels, preds)\n",
    "print(\"Metrics:\", metrics)\n",
    "\n",
    "conf_matrix = confusion_matrix(val_dataset.labels, preds)\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "class_report = classification_report(val_dataset.labels, preds, target_names=[\"NO\", \"YES\"], digits=4)\n",
    "print(\"\\nReport:\")\n",
    "print(class_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnE2FSCX_sdN"
   },
   "source": [
    "### Plot confusion matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "U6dTAmpJ_sdN",
    "outputId": "103f5530-7c0d-47bb-d070-9fd1cba80713"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"No\", \"Yes\"],\n",
    "            yticklabels=[\"No\", \"Yes\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2VsbMNKV_sdO"
   },
   "source": [
    "### Text and probability of False positive and False negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ogwz2zF_sdO"
   },
   "outputs": [],
   "source": [
    "def false_positive_false_negative(\n",
    "    y_true: List[int], y_pred: List[int], pred_probs: List[float], texts: List[str]\n",
    ") -> Tuple[List[Tuple[float, str]], List[Tuple[float, str]]]:\n",
    "    \"\"\"\n",
    "    returns incorrect predictions from a binary classification model.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true (list[int]): True labels for each sample (0 or 1).\n",
    "    - y_pred (list[int]): Labels predicted by the model (0 or 1).\n",
    "    - pred_probs (list[float]): Probabilities assigned by the model to the predicted class.\n",
    "    - texts (list[str]): Text of each sample.\n",
    "\n",
    "    Returns:\n",
    "    - false_positives (list[tuple[float, str]]): List of tuples for false positives.\n",
    "    - false_negatives (list[tuple[float, str]]): List of tuples for false negatives.\n",
    "    Each tuple of both lists includes the probability and the text of the sample.\n",
    "    \"\"\"\n",
    "\n",
    "    false_positive = []\n",
    "    false_negative = []\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == 1 and y_true[i] == 0:\n",
    "            false_positive.append((pred_probs[i], texts[i]))\n",
    "        elif y_pred[i] == 0 and y_true[i] == 1:\n",
    "            false_negative.append((pred_probs[i], texts[i]))\n",
    "\n",
    "    return false_positive, false_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s2hE0MNX_sdO",
    "outputId": "a4aef345-22cf-4b2d-9317-f2723136e0d4"
   },
   "outputs": [],
   "source": [
    "fpositive, fnegative = false_positive_false_negative(y_true = val_dataset.labels, y_pred = preds, pred_probs = probs, texts = val_dataset.texts)\n",
    "\n",
    "print(\"False positive:\", len(fpositive))\n",
    "# for i,s in enumerate(fpositive):\n",
    "#     print(i,'S', s[0], s[1])\n",
    "print(19, fpositive[19][0])  # highest prob\n",
    "print(fpositive[19][1])\n",
    "print(22, fpositive[22][0])\n",
    "print(fpositive[22][1]) # lowest prob, almost 1/2\n",
    "print(33, fpositive[33][0])\n",
    "print(fpositive[33][1]) # almost highest prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIhdkSqRq7zf",
    "outputId": "eb3cadc3-ccae-4eb2-d7d6-196c955f0dcc"
   },
   "outputs": [],
   "source": [
    "print(\"False negative:\", len(fnegative))\n",
    "# for i,s in enumerate(fnegative):\n",
    "#     print(i,'S', s[0], s[1])\n",
    "\n",
    "print(22, fnegative[22][0])  # highest prob\n",
    "print(fnegative[22][1])\n",
    "print(0, fnegative[0][0])\n",
    "print(fnegative[0][1]) # lowest prob, almost 1/2\n",
    "print(9, fnegative[9][0])\n",
    "print(fnegative[9][1]) # prob somewhere in the middle - 75%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jW8mPP4o_sdP"
   },
   "source": [
    "### Select some samples to analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SanVgDW_sdP"
   },
   "outputs": [],
   "source": [
    "fpositive_samples = [fpositive[19][1], fpositive[22][1], fpositive[33][1]]\n",
    "fnegative_samples = [fnegative[22][1], fnegative[0][1], fnegative[9][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AIOzyTvo_sdP"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import tee, count\n",
    "\n",
    "def uniquify(seq, suffs = count(1)):\n",
    "  \"\"\"Make all the items unique by adding a suffix (1, 2, etc).\n",
    "  `seq` is mutable sequence of strings.\n",
    "  `suffs` is an optional alternative suffix iterable.\n",
    "  \"\"\"\n",
    "  not_unique = [k for k,v in Counter(seq).items() if v>1]\n",
    "  suff_gens = dict(zip(not_unique, tee(suffs, len(not_unique))))\n",
    "  for idx,s in enumerate(seq):\n",
    "      try:\n",
    "          suffix = str(next(suff_gens[s]))\n",
    "      except KeyError:\n",
    "          continue\n",
    "      else:\n",
    "          seq[idx] += suffix\n",
    "\n",
    "def deduplicate(explanations):\n",
    "    for i in range(len(explanations)):\n",
    "        tokens = explanations[i].tokens\n",
    "        uniquify(tokens, (f'_{x!s}' for x in range(1, 100)))\n",
    "        explanations[i].tokens=tokens\n",
    "    return explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vgS99ydb0H5H"
   },
   "outputs": [],
   "source": [
    "\n",
    "from ferret import Benchmark\n",
    "from IPython.display import display_html\n",
    "\n",
    "model_es = mixmodel\n",
    "bench = Benchmark(model_es, tokenizer)\n",
    "\n",
    "def explain_this(benchmark, sentence, target):\n",
    "    explanations = benchmark.explain(sentence, target=target)\n",
    "    explanations_de = deduplicate(explanations)\n",
    "    explanation_evaluations = benchmark.evaluate_explanations(explanations_de, target=target)\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Class:\", target)\n",
    "    tble = benchmark.show_table(explanations_de)\n",
    "    tble2 = benchmark.show_evaluation_table(explanation_evaluations)\n",
    "    display_html(tble.to_html(), raw=True)\n",
    "    display_html(tble2.to_html(), raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWcdsJf9_sdQ"
   },
   "source": [
    "### Show explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9a46cf710a554746a34c8f10165e4b97",
      "4aa889f1f40b402f99e0ef1583f990c0",
      "d0350a818324405e994f59ffae30323d",
      "92285908035e4459b01fb978a36325ff",
      "b2120fcab5f14c438356ba66996c580e",
      "ab2ce3c235e8464b8fcf5245dab213c1",
      "c15265fe02534de389c3f8044a754712",
      "c83d9caedf274d19b609824899634ca3",
      "77ace456b6814ede854cc048512be8e5",
      "d616dd15e4994428bcda666d7e8275e5",
      "ad787bedb3ff468e8a7d831692fab284",
      "8fb5880b81834c21bc16931e45a3b6f6",
      "061f3d4c7ee5422b8395ccb7c5efc5fd",
      "86cf57f1f64e420caf368feeec32b301",
      "497dd36cc5ce4d05ac37de7a20a885d6",
      "3a9cc53523bc4d0aa280256c83ef4d7d",
      "c84afe6bdf0b46aa942032c07e69e4f1",
      "cd8b6cc699654493b3dc9efef0a4e2d5",
      "346f2bdb74f74610aa35b658daa0c1eb",
      "b16ddbb00d21464ba9281f95daeb924c",
      "5e75356bb7034c898ede5808e1b88789",
      "380cd4e77b5e4075bb044484396cf866",
      "50c340afd26c43e39b6dd4eb363d2a10",
      "4fd6abfca7fe48da8dbe22ccdd54f28b",
      "c0a22e9bc0054fb680de500de0b402ab",
      "550f3365adfc41eeb806f4f52bcd4e04",
      "fbd968884bc24192a3b13220f7477cfe",
      "9f9f11a4953146558f686b4318298717",
      "dc39c317d1eb46779920737b1940d304",
      "d0da727d90eb4dc38e77045038ad4fe0",
      "59975328a5da4652beb304c89026f818",
      "7ed1eba9c14e4ab6bf91651cb55a73fc",
      "ac3392c530bf48088607526fe33294a1",
      "850ebfe5d7b944f8860bbbcc3b59366d",
      "fe19f1586c374ba1aaf87931621719b2",
      "c2322669f8ef4e9d8ffdfdf18a9286a3",
      "e608d594ac6a4be48330ff8a9691ba94",
      "617a6e1bc92e4a6b92f89285eeaca096",
      "41588c9deb744e8ab3f60b8cd770d7db",
      "c198c18ba0e7454a860c538ad70920cb",
      "a7d7825cf0984aa18e7fa99db97ae3d5",
      "6214fa131fe74d6d8fa93740a82f92fb",
      "454df301c0854361b9a2efe7f9d516b4",
      "a43d7c23032547a6a73330357d67df13",
      "0cfc1b54bf9e4f1b83b43c1a672a11fa",
      "2dce920488774a74a77abdf11fe28ee3",
      "7b91edb44c754db19dfa6243ffddcba1",
      "7ad5aaf63cf6464aa77d0749f0846dda",
      "130dd45f22394146bf314fbcbb6ae133",
      "85178eb5be254113ba90a06719a358c0",
      "45296e54da194b40bf63667f782d1a63",
      "0ee0c6a8e688451e88bc93e7a318d9f2",
      "fdf07803d10e4f6488876271f5c47b4a",
      "b77200affd6d4535a4988f770e7ca40c",
      "6ca943978e55493ab34d6fce5f56f59e",
      "45b63cd65e8d463f965111d4104c4b0a",
      "e5f695c2b6194444a0ce66a86d622ddb",
      "fe7142a5f8b44ccf881dc3065e71da6c",
      "776073739ebf4879a902c8ca71351b48",
      "2e15963a2c5748f4a18d6ecbf494d18d",
      "8401bdc581df466e86840a4218c21276",
      "72d830b72c4f421583f1c701c708106f",
      "3aa5d26ce07f45468803bc7eb8d89dba",
      "bc62691acf4141f5a8e098d47fd47865",
      "d6cfb2eabac745caa2d2e0198d0b6ceb",
      "11cae109a991465880f16451a1e615ed"
     ]
    },
    "id": "w_GZkBUt_sdQ",
    "outputId": "abcea7c6-6bf8-4d72-dd60-f7a0c715f0db"
   },
   "outputs": [],
   "source": [
    "for sample in fpositive_samples:\n",
    "    print(\"False Positive:\")\n",
    "    explain_this(bench, sample, 1)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FE_kN83KPVzX"
   },
   "source": [
    "**Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmAYSxQwSAuw"
   },
   "source": [
    "**Spanish - False positives**\n",
    "\n",
    "Based on the SHAP and LIME analysis, specific keywords like \"mujer,\" \"mujeres,\" \"feminismo,\" and \"feminista\" appear to strongly influence the model's prediction. The model gives high importance to these terms, leading it to classify tweets as sexist mainly based on their presence. This strong keyword association, without taking into account the surrounding context or intent, likely causes these tweets to be misclassified as false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "88bac3d68d2541d1b077ff27e59264d0",
      "c95e08307b9a4315b95b4155fdaf4317",
      "885dd9fea95544e7826259741c9ac35d",
      "85e77002c97044dfa15ede1c14d553e5",
      "239560663a8f4b01b849e7fa0120da12",
      "8ba2ce9a0c514cceb64f280d910cb805",
      "8bb4cfde34bf4f40b997e37c233565b3",
      "8250954d12964c7b8394f2031983741d",
      "86b34a50ef47435e8b9a90838a30ecb5",
      "6b4f14bd31544a319dee083ec8e45c9f",
      "49fafc3220db4dcd98534ab9442c8cab",
      "9a00fa5583234ea690c8570b7f101567",
      "3a78f2df9cca42ccbded47c91f75d613",
      "63170577a3f1447ab52b660879088816",
      "14535820640046b8a52c641f2515f135",
      "2ef8e74bb884417ab94a214225ddb049",
      "50f3536b7184471d9cc0e990be8d34ad",
      "988856d0205b449faea7d7593a7e63c6",
      "ed23f77558f14efcac1a21c88fbccce6",
      "b4ffb724f5bf4ca2928f051307ec4619",
      "8ef597b0be5f4d4a966f4451f47b1496",
      "2beba1dc6cc945c09e7a31c0f7113021",
      "086d3fa5a6554603b97e4e48e334859e",
      "090f8b3ff322485885c3bbde49f2e012",
      "5817457a19104e509ee43ada821ac88a",
      "bcff36ecf716440cb7128fa9eb73a68d",
      "1eb3f0df12ee4e13a97730df32117c8b",
      "e7d7d39bc952482f9a52447b9d85e850",
      "b53e5818b382473599011abd13bd3f4f",
      "814442b3573c43f191bbf42282605bf9",
      "249d56e05fb44463af90fe614081b1b7",
      "7605c479fb5145d2ae795e693fbd511c",
      "ca9e5e6b4ad44c639bbb9df26a7b659e",
      "7bdce6bac77b4802ba6cb88245435480",
      "2f18542b0d7c46faadb40db2da2729dc",
      "aad9cf21e5834a2eb92a23517cba871e",
      "00c352e022b1424f982b5a2a240f8c94",
      "1fa01394feeb4c59b0233ba2c395bc8c",
      "d9397212209f4c32af3aad7e622757a2",
      "9c913157b94243d2aec78f8e49e159b4",
      "f3d6aec99ce54c768ba937ab11c982ab",
      "ec05ec5881b042d1ac000243c765b850",
      "49b7751f07f64561a580cdadf2543a57",
      "a290321445d54ce99684abaf1eeeed0c",
      "d012516de6ba4615adcbdac8317546e9",
      "bf1b2ac868c94962bc90fadbc04e0474",
      "05482305557a4c0c922e8e24a5862754",
      "548dc106bb1e44c5b1e1c0ec0f8a03aa",
      "bf06d65a5e244ac58ab43ec14aba2a3e",
      "f49ca91ea2be4a88923fdad5f26e382e",
      "1dd3d3fe65594a48a9d5b9d5fc7381f1",
      "572a9bdc28324cdf83207a548f96aa1a",
      "18a01c47897a47f684e04109df995838",
      "b70332f4f16248238de88494fd6be3ba",
      "5ccd721d457a4a8d8004307588b5cbe0",
      "cfbf068275e3445c90518ba38702822b",
      "3b866673cd68448e816557b1621a3169",
      "b6c93e3b5afd47c7931af3db5ee81b5c",
      "12d9728b147146aab947b34528503399",
      "304081d9199047b49667bfdaca1c11b3",
      "a17aacf30dd74107b5052f0b948dfe2f",
      "b1e622e821a141f094473f3b885d33d5",
      "a3d3daba73384af09da54a12e550d0f5",
      "270708c3f154448c81222c713287e060",
      "85cf13ed04444cdc9c88d6151f14f9d2",
      "0ad25700b1aa416cb9b127e8ed07de87"
     ]
    },
    "id": "qKjHW17Syc2z",
    "outputId": "ee97bc1b-a45d-43fc-f086-c14acc396916"
   },
   "outputs": [],
   "source": [
    "for sample in fnegative_samples:\n",
    "    print(\"False Negative:\")\n",
    "    explain_this(bench, sample, 0)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaYgsOvfQVz5"
   },
   "source": [
    "**Spanish - false negatives**\n",
    "\n",
    "Looking at the SHAP and LIME values, it seems the model doesn't put enough importance on the parts of these tweets that signal they should be sexist. It looks like, none of the tokens have significant values, which means, that the model \"thought\" that none of the words is out of average tone of the sentence and didn't point it out.\n",
    "\n",
    "This means the potentially relevant content gets overlooked or canceled out, leading to them being classified incorrectly as non sexist.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ARDG7TNXr3yD",
    "outputId": "1de4ebb2-2e84-4d44-9326-7fe8c6ea341c"
   },
   "source": [
    "# DO IT IN ENGLISH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqr8Sme6M8_C"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601,
     "referenced_widgets": [
      "ca8cbd6b38a649c98bb7c32e6d25b6c6",
      "7dd27d532b4d470caf24262c60bc510b",
      "f49d3b422a1644ec8dc239197b142527",
      "a226e2e0b52a44a0acfb6fb37c566f43",
      "387d4af7d4d54d7087493ba2daa741d1",
      "3c1f4444cfed46e9be64ca353f3e5550",
      "8be7e4aa20a74de18070c89e099edbf4",
      "2d0220f29d784ea0a3f73a43cd42e083",
      "b1e0f27dd0194d56b2a7a45e1ba4f29b",
      "6448bb3523a140bda029f18fc1185626",
      "eac77e6c85f440208caf0b7c6a19e86a",
      "010fa4d91c0b471eb2aff697e8295925",
      "1dce8cbf7abb44d3835275ff0ceebb4d",
      "9e0e7126375e4e0e84673b7f77fe5d5d",
      "e7e5c7858f534364b9d3482705124e18",
      "7cececcef840446bbfae558dec0ab99f",
      "b3abb76992114a6b834d086bb18e0271",
      "1009b39aa4af4f49a8bf3360323ab147",
      "f473cbc7b2e840009e3d0606b053e809",
      "c0b403dfe344423c866b769393ade578",
      "5f8a222dbbbd4526aa23c45499a59453",
      "10721bf58d7a4d919a3ae10a165e1f52",
      "200b9e82d0174bd78218240e719a4394",
      "39ed73c3bb864b7086df5a196021046d",
      "07dda8c9c796486ba22ff95f27bb63e5",
      "0e8b86a0d318459b9437fe45653bd97c",
      "736d24825305497eb5484406a4195e05",
      "14119f60e0034153a6894b0f12a51d94",
      "e324006bbd5e4dc1939a259e55a5840b",
      "59b3bbd0a93541948b22a9c20d52c978",
      "13b3929a3d074754a102dbb44be31ab8",
      "fa4bc3a39575426cbc6681ecefe248fc",
      "55cf094d2bce40acad4a7df55c4cfb47",
      "cb4680b9ec774f458a6df4109c0b6cf8",
      "2af5667f6a354779bcf3190c439f7d4b",
      "7f02af5c4bea461a9874f896677917a9",
      "021d4b6d3d3b49caadf348c950adbe00",
      "b9f130c995104af6b697708e9daeec6e",
      "37b4c5868b3143d4b6f5cbf9de2b32e6",
      "8ba67f3f5ffd4bf68ea7f1e6f3af44b9",
      "1090ce5e34c3453fac24e67065d43869",
      "ba833490a277443e8d7d8801d3279526",
      "8e7aeab16cd74117b05eec46243823ef",
      "6a99f14edcd34d2abb21ccac9844e419",
      "ae9bc6da68c9464baa91240660404eb8",
      "ac4e7f8d8c904f5ebb6ea6c1fbb906e0",
      "1795bae2c9f34d5dbd9454c1bb012ca9",
      "8bd67ef661f8460e8441db209e6394a2",
      "6ae426e8690940a3a05622607644af20",
      "1fef91e99391448e8973793aaab8f1d9",
      "6a212432926d4b5f8324bebdc2f26f6c",
      "1bd7d75ade004ecfaa8d236baeefecee",
      "67ce20c66b574fc493623e86b8423c75",
      "69b2d77823be4cc6bd52e8cf9ccb5c60",
      "c4ac02b618a54d09ba5821eb462c21e9",
      "2c7504d454fc4386adc6b3636b7dbf80",
      "253c601934fa44b99066596ef4cb95bb",
      "b97991ae001143969c2888b7bf042c77",
      "b1237dbed3ce4718ae8d320638dacb21",
      "c68bc251489b450bbd4c09e3579d5867",
      "5c6fb67de087415daa7c949502f44787",
      "bd0bb2a0caac46038f1d2f98754b5031",
      "c9c0253b8a024b678ba9eaa4b1a66188",
      "c01e70c8fac848eca9e1516e0fba2469",
      "f097a185af714955b63f08669b91d165",
      "61a9bbdae733426ab454ac53c474fbab"
     ]
    },
    "id": "Kfk-KoQoM8Ws",
    "outputId": "d8938687-179d-4db0-81b4-b40f5d8409c5"
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"num_train_epochs\": 6,\n",
    "    \"per_device_train_batch_size\": 16,\n",
    "    \"warmup_steps\": 100,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"early_stopping_patience\": 2,\n",
    "    \"use_lora\": True,\n",
    "}\n",
    "modelname = \"roberta-base\"\n",
    "output_dir = \"./drive/MyDrive/best-finetuned-model-english\"\n",
    "\n",
    "model2, res = sexism_classification_pipeline_task1_LoRA( EnTrainTask1, EnDevTask1, None, modelname, 2, \"single_label_classification\", **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ClD_q4gm-Hw"
   },
   "source": [
    "### Predictions from best English model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "TSplKWciNMOR",
    "outputId": "f3edceea-b1cf-4b94-c269-3bb13577a887"
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "val_labels_encoded = label_encoder.fit_transform(EnDevTask1[2])\n",
    "\n",
    "tokenizer_en = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "val_dataset_en = SexismDataset(EnDevTask1[1], LabelEncoder().fit_transform(EnDevTask1[2]), [int(x) for x in EnDevTask1[0]], tokenizer_en)\n",
    "\n",
    "preds_en, probs_en = predict_op1(model2, val_dataset_en)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Tgd2RvGpG0j"
   },
   "source": [
    "### Evaluation of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGw11RvyqWcx",
    "outputId": "9d08e59d-a7a2-48be-a713-32c0229f5669"
   },
   "outputs": [],
   "source": [
    "metrics_en = compute_metrics(val_dataset_en.labels, preds_en)\n",
    "print(\"Metrics:\", metrics_en)\n",
    "\n",
    "conf_matrix_en = confusion_matrix(val_dataset_en.labels, preds_en)\n",
    "print(\"\\nConfusion matrix:\")\n",
    "print(conf_matrix_en)\n",
    "\n",
    "class_report_en = classification_report(val_dataset_en.labels, preds_en, target_names=[\"NO\", \"YES\"], digits=4)\n",
    "print(\"\\nReport:\")\n",
    "print(class_report_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPlQfvvqnaWj"
   },
   "source": [
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "U5OCkvRlnRpI",
    "outputId": "f46513fa-956f-4ed0-c84d-6bc9449250e0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(conf_matrix_en, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=[\"No\", \"Yes\"],\n",
    "            yticklabels=[\"No\", \"Yes\"])\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Prediction\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B4GbVZ1mntLu"
   },
   "source": [
    "### False negatives and positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_OWO5-S6nrOp",
    "outputId": "526a30a5-592b-489c-bb8c-30d5b4c4bc51"
   },
   "outputs": [],
   "source": [
    "fpositive_en, fnegative_en = false_positive_false_negative(y_true = val_dataset_en.labels, y_pred = preds_en, pred_probs = probs_en, texts = val_dataset_en.texts)\n",
    "\n",
    "print(\"False positive:\", len(fpositive_en))\n",
    "# for i,s in enumerate(fpositive_en):\n",
    "#     print(i,'S', s[0], s[1])\n",
    "\n",
    "print(40, fpositive_en[40][0]) # - highest\n",
    "print(fpositive_en[40][1])\n",
    "print(42, fpositive_en[42][0]) # - lowest\n",
    "print(fpositive_en[42][1])\n",
    "print(26, fpositive_en[26][0]) # - middle\n",
    "print(fpositive_en[26][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cexugc_YnrpL",
    "outputId": "71913f6b-7d48-4468-f178-9c8ebc0ffeb0"
   },
   "outputs": [],
   "source": [
    "print(\"False negative:\", len(fnegative_en))\n",
    "# for i,s in enumerate(fnegative_en):\n",
    "#     print(i,'S', s[0], s[1])\n",
    "\n",
    "print(22, fnegative_en[22][0]) # highest\n",
    "print(fnegative_en[22][1])\n",
    "print(10, fnegative_en[10][0]) # lowest\n",
    "print(fnegative_en[10][1])\n",
    "print(16, fnegative_en[16][0]) # middle\n",
    "print(fnegative_en[16][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "izH_Ic6cn6QJ"
   },
   "source": [
    "### Select own samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gaHhovAnr_6"
   },
   "outputs": [],
   "source": [
    "fpositive_samples_en = [fpositive_en[40][1], fpositive_en[42][1], fpositive_en[26][1]]\n",
    "fnegative_samples_en = [fnegative_en[22][1], fnegative_en[10][1], fnegative_en[16][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3V8PHqooS3E"
   },
   "outputs": [],
   "source": [
    "bench_en = Benchmark(model2, tokenizer_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VAnjN9soX3_"
   },
   "source": [
    "### Show explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "63730ac652c145edafa41980267c136b",
      "306b020de0d14ff089871807639ff923",
      "a7aabe659a034c61b3b564da6fa0c007",
      "eda2e7e7fd43478b8a372c34cca78b7a",
      "b17a501838f34a28b64eef85719bee69",
      "39a96b3753804dc794cdc0298d7ce422",
      "874858d263744ba284d4b0b6d81ebfda",
      "6006aee138754ea191188797f7535e49",
      "b40f9b9eaf5846429e2b669f48be1eae",
      "c2e4f9c1f11b440f8d9041aaea2c0436",
      "e4619e53bcbf43dd9186d62e3f5db9f3",
      "99bdbb30845e4c5cbed080528bd2c65d",
      "c7a7d584f9a24ebfb59a0fd4b8dd2443",
      "25542fc76bb44a48b5d09bb4f7a268cd",
      "681b34e423c04ce088ca142b6700da07",
      "b6b702b07da240adb34a97c54b9c0409",
      "2557445c6c474ddca54f086f7ce8baa4",
      "c5af2fc2eea246aaa62af90a81238d17",
      "bce7b7e27c7644c8b12baf0f861a9fac",
      "8659fb5ef2a949dfbe78ee6ecac61bb3",
      "4314ed95441449109dcc7ba7bc331881",
      "e675f651a9e04b96aba1cd1a40963075",
      "964d908abd3143fd92da15558688b5f9",
      "48f24b0966764cccab0b74f3ebdc950f",
      "19d7e3d4a0fb4d00bd47d1223f51b508",
      "cc9131696a7b43d1bae1450e06225619",
      "4c83b1be29e2411bbcdde9d5ac558420",
      "dae33e93d8724cada4c7bbe70f236176",
      "cfa8512a8c3448aab5f6ac6ae0ae749b",
      "0e4373a664954302b69ef037f4149c3a",
      "877b98011ba844948178243d671f63fe",
      "c86a786b5adf439a802c1b99b67001a8",
      "aa5975c8e8e34a1a9afbb6fbf471a8f3",
      "169324679ec645cda046c09058ad6695",
      "e1a2a61a8e794a419b4f55f58fee1080",
      "3ab8668491854e03b38a727215089c52",
      "a205115d0a114ec88997c689dc8bbb00",
      "dd5830f4c34e4be09742c0e89f3c92de",
      "5cd447b1fe134953974f10710476417d",
      "9ba53206627a4db0853b4fe301f86f3f",
      "e2de5b4d41a245df8884aeec97c60729",
      "fd9f864fddd04ce6942002f55bb20b13",
      "c05596a0b7424ff08e3a0d8436798c85",
      "56fb1cca33074674bfdc83de609c9a83",
      "30912e513813479b88ef56ad2f2a7248",
      "1b55795093594d4e92f67f8302589316",
      "afc9ddeaf5ae4ef487e4e4f9f100ffe3",
      "cfa731c26c46419eabd3fc688c7f0cbd",
      "4c542028c10047d78c5121384bde4130",
      "46f16139b08e42088c4a9abf085e6ff4",
      "6bb9e9cfba0049e7a87e8eb22b737fe6",
      "52a724d4efce4db483c1f4a46096b20f",
      "5e5145b4f479468fa451f87d76d67188",
      "0bbf48dd8e0d46edb79c7e749cf4fc66",
      "075e9f50a85045108acaffeae9caa63d",
      "11ad8b199abf447cab65b3f6be68abd5",
      "4b5b1535159c4789979fa142aa26ff8b",
      "81eee8c3726e46ba81b26cc374657e21",
      "2e09c1ada71143e59b220bf2446c74aa",
      "7e8583c5771c4205a1b37865a53227ac",
      "cb3297d828a64587b0ba9ab233d481e6",
      "9b9940998ec84307890c641a52c518d7",
      "6bde1d5638e4447797c8a2758a7b4e64",
      "c77d974d9ee04a93af757270c611596c",
      "023dcae46a884daab9a853e7c37d9302",
      "9244f138520841d59f6c79752578ef49"
     ]
    },
    "id": "GD4gc3kqoULi",
    "outputId": "5bebb517-b476-41a1-fcea-ac5af80888bb"
   },
   "outputs": [],
   "source": [
    "for sample in fpositive_samples_en:\n",
    "    print(\"False Positive:\")\n",
    "    explain_this(bench_en, sample, 1)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A05SiAbm8RUt"
   },
   "source": [
    "**English - false positives**\n",
    "\n",
    "Based on the SHAP and LIME values, the model's misclassification stems from its strong positive reaction to key words. Confirming a previous pattern, terms like \"women\" significantly drive the prediction towards sexism classification. Additionally, words describing negative social concepts such as \"envy,\" \"gossip,\" and even explicit terms like \"penis,\" especially when linked to \"men,\" also heavily contribute to this false positive classification. This shows the model associates sexism with specific gendered nouns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "8364656fa9b6497683d4aec9aa21aa14",
      "f7f0fe52c57343d8ab4a72a1f935491f",
      "2cc69b11a76d42bf93665bfe52a89bce",
      "88a22fc7a95b46ea973796f3343d66b1",
      "10f1e3980fa24748b067fd8715b73b3a",
      "06adbdc8475a42ad898f1c2dcf5701e2",
      "d21f86dbf9bd4d4fbe4246b7a4ded94f",
      "2d4883bcd54a43a1a568b569b3ff6100",
      "083785739a0d4917bc38640ebd589562",
      "2ff4b12e7b8245279fa8a66971a851b3",
      "32d57a059e4a4f14a3932c9dfe7f51a5",
      "02b68416369840faab4136f239aac97a",
      "bc7cd76354154c3ab2cc8dbc40bad543",
      "be44d3f5a65748f28b6fb07af50667ff",
      "580e736cf0ec493fa738f8d37c954059",
      "e1901ced0f4740228d3e39d4185981a8",
      "b3d3902a37ea4b0cab9179ceb2e76ff1",
      "6503934d3389472bb5027bf51f9b1ea6",
      "108fae7b4eec44829150b4d4fe8cc273",
      "6d4cab2c54784afea5c193fec78459e7",
      "958bd6a5c9654b7ab9d992aea6418ab9",
      "7f16c1ab359c4c0c9a89ea31d3b98eb4",
      "f84ec01016074d639948f5833c9a37ed",
      "4fc297337ff548d7a153886cef4c4ac4",
      "3d1b057c5057430dbc90d4f2c3cde795",
      "5830de2604e442368206637ac04e7538",
      "83f274e8e3174d8caead89b338f0b6bb",
      "85b8f3c5c2214fdcaaf6bb53f042bd81",
      "b89f5d21d3c44f929a3f0bc30bc3a8f0",
      "d26a4da083f7413892041398b412f213",
      "8add714f715347158a931c8544fb3f1b",
      "687f994e75394287b01306dcecf5513f",
      "81b23fc1f4324d7f8f41c6d86c7c15e5",
      "62e2809d693a4dca8cf9d1186e302dc6",
      "bc2dc89230744abf94366a001ed4dbb6",
      "f90ee01027b4454c9f531dc513836746",
      "9bfcead0550246268e9f66825e41bef5",
      "ae86472894724d18840f5f03816da6d7",
      "263c92fe259b4f4ea145c3514bd083be",
      "69f40cf5d7074267a25d908baaa969f2",
      "b3a090b21c564e7b89f18dc843ad60a4",
      "c823f4822adc49248924d72b15d20f21",
      "db5157404fd4424991632ba43cfc3814",
      "d3e79c90716f404e9fc35404fb72cfa5",
      "3927e7cd44ee415284e4206713d077c4",
      "f9b3e2534b1a42bc8c1c699eaf58c2cc",
      "aa456bb10cdb4d188051e2e5821bb678",
      "992a6a01d9ea49fc81571eb3298e00a5",
      "d829ddb6def046a8a37cfcf5ae704e11",
      "0e844301dd534f1fad22c6e2f7fdcfff",
      "e495243dab6847019b834733b8c9e470",
      "e661096ca77b45b0a2da2efa66b9f920",
      "6a4a4d8cce3448ab99dede832bc7113f",
      "228de2285af34c549e189ce3cfa29dd5",
      "1a0e065433bf4029aaf9369f5f9bebc9",
      "59580f0f2d31472aab83ff0e7fe2cf31",
      "3bf14378f27a4dd8ba60b63ae72f060c",
      "1f07f92a7b024c8086e693be7def6386",
      "74357bafe1904f0b84215b2cd725e908",
      "77f173b21cca4d8da85ac7876d2ad5c3",
      "0391b208ba084c4eab56d5373ca55a7f",
      "c9f4158fe3be4eb0bdeb3760095907a9",
      "ebd69928efe64251aecf82d47ee80a2d",
      "00a092644da048f1a3caa3157ceb1849",
      "ad9c9afef78c45399b7c78f9096274e7",
      "be40e2cbeada4b4eb11ed04d9323fccd"
     ]
    },
    "id": "Q746RZNaoTvt",
    "outputId": "80ed74bc-eba3-4782-9662-4430d919a603"
   },
   "outputs": [],
   "source": [
    "for sample in fnegative_samples_en:\n",
    "    print(\"False Negative:\")\n",
    "    explain_this(bench_en, sample, 0)\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wuLebyhNieh"
   },
   "source": [
    "**English - false negative**\n",
    "\n",
    "Based on the SHAP and LIME values, it appears the model didn't strongly recognize the phrases indicating sexism. The words you'd expect to have high positive influence actually received low or mixed scores. Instead, a number of other words were given negative importance, effectively pulling the prediction down to non sexism and causing these tweets to be missed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odAn8wycAKY5"
   },
   "source": [
    "**Overall**\n",
    "\n",
    "**False positives** at both languages are mainly because of the model, focusing on keywords and overlooking the omportant context, in which the words are\n",
    "\n",
    "**False negatives** on the other hand don't show significantly high SHAP and LIME values, which shows, that there are no few words, pushing the prediction to non sexism. This usually happens, when the tweets are more complex or the sexism is only reported, which might suggest, that the mode has not seen enough of reported sexism tweets."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
